线性回归
-------

## 由来
通常我们学习机器学习都是从线性回归模型开始的。线性回归模型形式简单、易于建模，但是我们可以从中学习到机器学习的一些重要的基本思想。   
    
回归一词的由来：
> 这个术语是英国生物学家兼统计学家高尔顿在 1886 年左右提出来的。人们大概都注意到，子代的身高与其父母的身高有关。高尔顿以父母的平均身高 X 作为自变量，其一成年儿子的身高 Y 为因变量。他观察了 1074 对父母及其一成年儿子的身高，将所得(X, Y)值标在直角坐标系上，发现二者的关系近乎一条直线，总的趋势是 X 增加时 Y 倾向于增加，这是意料中的结果.有意思的是,高尔顿对所得数据做了深入一层的考察，而发现了某种有趣的现象。
>高尔顿算出这 1074 个 X 值的算术平均为 68 英寸(1 英寸为 2.54 厘米)，而 1074 个 Y 值的算术平均为 69 英寸，子代身高平均增加了 1 英寸，这个趋势现今人们也已注意到。以此为据，人们可能会这样推想：如果父母平均身高为 a 英寸，则这些父母的子代平均身高应为 a+1 英寸，即比父代多 1 英寸。但高尔顿观察的结果与此不符，他发现：当父母平均身高为 72 英寸时，他们的子代身高平均只有 71 英寸,不仅达不到预计的 72+1=73 英寸，反而比父母平均身高小了。反之,若父母平均身高为 64 英寸，则观察数据显示子代平均身高为 67 英寸，比预计的 64+1=65 英寸要多。
>高尔顿对此的解释是：大自然有一种约束机制，使人类身高分布保持某种稳定形态而不作两极分化。这就是种使身高“回归于中心“的作用。例如，父母身高平均为 72 英寸，比他们这一代平均身高 68 英寸高出许多，“回归于中心”的力量把他们子代的身高拉回来些：其平均身高只有 71 英寸，反比父母平均身高小，但仍超过子代全体平均 69 英寸。反之，当父母平均身高只有 64 英寸，远低于他们这代的平均值 68 英寸时，“回归于中心”的力量将其子代身高拉回去一些，其平均值达到 67 英寸，增长了 3 英寸，但仍低于子代全体平均值 69 英寸。
>正是通过这个例子，高尔顿引人了“回归”这个名词。


## 概念
在统计学中，线性回归（英语：linear regression）是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归（multivariable linear regression）。
   
在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。最常用的线性回归建模是给定 X 值的 y 的条件均值是 X 的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定 X 的条件下 y 的条件分布的分位数作为 X 的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定 X 值的 y 的条件概率分布，而不 X 和 y 的联合概率分布（多元分析领域）。
   
线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。
   
线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚。相反，最小二乘逼近可以用来拟合那些非线性的模型。因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。


## 应用
1. 如果目标是预测或者映射，线性回归可以用来对观测数据集的和 X 的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的 X 值，在没有给定与它相配对的 y 的情况下，可以用这个拟合过的模型预测出一个 y 值。
2. 给定一个变量 y 和一些变量 X_1, ..., X_p，这些变量有可能与 y 相关，线性回归分析可以用来量化 y 与 Xj 之间相关性的强度，评估出与 y 不相关的 X_j，并识别出哪些 X_j 的子集包含了关于 y 的冗余信息。


## 损失函数 / 代价函数 / 目标函数
1. 损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。   
常用的损失函数包括：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等

2. 代价函数(Cost Function)：度量全部样本集的平均误差。   
常用的代价函数包括均方误差、均方根误差、平均绝对误差等

3. 目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。   
具体可参考：[深入理解机器学习中的：目标函数，损失函数和代价函数_qq_28448117的博客-CSDN博客](https://blog.csdn.net/qq_28448117/article/details/79199835)



## 一元线性回归
方程：
```
    y = mx + b
```
该方程描述了变量 y 随着变量 x 的变化而变化，方程从图形上看，是一条直线，如果建立好这样的模型，已知 x 那么我们就可以得到预测的 $\hat{y}$ 值了。统计学家给变量带上了一个小帽子，表示这是预测值，以区别于真实观测到的数据。方程只有一个自变量 x，且不含平方立方等非一次项，因此被称为一元线性方程。

![](https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/linerregression/lr-years-of-education.png)

在对收入数据集进行建模时，我们可以对参数 m 和 b 取不同值来构建不同的直线，这样就形成了一个参数家族。参数家族中有一个最佳组合，可以在统计上以最优的方式描述数据集。那么监督学习的过程就可以被定义为：给定 N 个数据对 (x, y)，寻找最佳参数 $m^{*}$ 和 $b^{*}$ ，使模型可以更好地拟合这些数据。

上图给除了不同的参数，到底哪条直线是最佳的呢？如何衡量模型是否以最优的方式拟合数据呢？机器学习用损失函数（loss function）的来衡量这个问题。损失函数又称为代价函数（cost function），它计算了模型预测值 $\hat{y}$ 和真实值 y 之间的差异程度。从名字也可以看出，这个函数计算的是模型犯错的损失或代价，损失函数越大，模型越差，越不能拟合数据。统计学家通常使用 $L(\hat{y}, y)$ 来表示损失函数。

对于线性回归，一个简单实用的损失函数为预测值与真实值误差的平方。

![](https://juejin.im/equation?tex=L(%5Chat%7By_i%7D%2C%20y_i)%20%3D%20(%5Chat%7By_i%7D%20-%20y_i)%5E2%20%5Ctag%7B1%7D)

公式 1 来表示单个样本点上预测值与真实值的误差。

![](https://juejin.im/equation?tex=L(%5Chat%7By%7D%2C%20y)%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5EN(%5Chat%7By_i%7D-%20y_i)%5E2%20%5Ctag%7B2%7D)

![](https://juejin.im/equation?tex=L(%5Chat%7By%7D%2C%20y)%20%3D%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5EN%5B(mx_i%20%2B%20b)%20-%20y_i%5D%5E2%20%5Ctag%7B3%7D)

公式 2 表示将数据集的所有误差求和取平均，再在其基础上代入公式
![](https://juejin.im/equation?tex=%5Chat%7By%7D%3Dmx%20%2B%20b)，得到公式 3。

![](https://juejin.im/equation?tex=m%5E*%2C%20b%5E*%20%3D%20%5Cmathop%7B%5Carg%5Cmin%7D_%7Bm%2C%20b%7DL(m%2C%20b)%20%3D%20%5Cmathop%7B%5Carg%5Cmin%7D_%7Bm%2C%20b%7D%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%5B(mx_i%20%2B%20b)%20-%20y_i%5D%5E2%20%5Ctag%7B4%7D)

公式 4 中 `arg min` 是一种常见的数学符号，表示寻找能让 L 函数最小的参数 $m^{*}$ 和 $b^{*}$ 。

![](https://user-gold-cdn.xitu.io/2019/11/28/16eb0f6da8c9366b?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

误差从图形上看是一条线段，平方后就形成了一个正方形，将正方形的面积求和再取平均，就是公式 3 的损失函数。所有的正方形的平均面积越小，损失越小。对于给定数据集，x 和 y 的值是已知的，参数 m 和 b 是需要求解的，模型求解的过程就是解公式 4 的过程。以上就是最小二乘法的数学表示，“二乘”表示取平方，“最小”表示损失函数最小。

有了损失函数，机器学习的过程被化解成对损失函数求最优解过程，即求一个最优化问题。那为什么只能用取平方，不能用别的办法，比如取绝对值呢？中学的时候学过，平方函数有最小值，能保证后面求解参数的时候，用求导的方式快速找到最小值，而绝对值函数则不具有这个属性。


## 最小二乘法参数求解
对于公式 4 中的函数，可以对 m 和 b 分别求导，导数为 0 时，损失函数最小。

![](https://juejin.im/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20m%7DL(m%2C%20b)%20%3D%20%5Cfrac%7B2%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5ENx_i(mx_i%20%2B%20b%20-y_i)%0A%5C%5C%3D2m(%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7Bx_i%7D%5E2)%20%2B%202b(%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5ENx_i)-2(%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7Bx_iy_i%7D)%5Ctag%7B5%7D)

![](https://juejin.im/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20b%7DL(m%2C%20b)%3D%5Cfrac%7B2%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5ENmx_i%2Bb-y_i%0A%5C%5C%3D2m(%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7Bx_i%7D)%20%2B%202b%20-%202(%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7By_i%7D)%20%5Ctag%7B6%7D)

公式 5 和 6 是损失函数对 m 和 b 进行求偏导。公式 5 和 6 是损失函数对 m 和 b 进行求偏导。

![](https://juejin.im/equation?tex=%5Cbar%7Bx%7D%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7Bx_i%7D%20%5Cqquad%20%5Cqquad%20%5Cqquad%20%5Cbar%7By%7D%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7By_i%7D%20%5Ctag%7B7%7D)

![](https://juejin.im/equation?tex=s%5E2%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7B%7Bx_i%7D%5E2%7D%20%5Cqquad%20%5Cqquad%20%5Cquad%20%5Crho%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7Bx_iy_i%7D%20%5Ctag%7B8%7D)

结果中重复出现了一些关于 x 和 y 的求和项，给定数据集，这些求和项可以通过计算求出来，是常数，可以用公式 7 和 8 表示。

![](https://juejin.im/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20m%7DL(m%2C%20b)%20%3D%200%20%5CRightarrow%20ms%5E2%20%2B%20b%5Cbar%7Bx%7D%20-%20%5Crho%20%3D%200%20%5C%20%5Ctag%7B9%7D%5C%5C)

![](https://juejin.im/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20b%7DL(m%2C%20b)%20%3D%200%20%5CRightarrow%20m%5Cbar%7Bx%7D%20%2B%20b%20-%20%5Cbar%7By%7D%20%3D%200%20%5Ctag%7B10%7D)

当导数为 0 时，可以求得最小值，即由公式 9 和 10 可以得到最优解 m 和 b。

![](https://juejin.im/equation?tex=m%5E*%3D%5Cfrac%7B%5Crho-%5Cbar%7Bx%7D%5Cbar%7By%7D%7D%7Bs%5E2-%7B%5Cbar%7Bx%7D%5E2%7D%7D%20%5Cqquad%20%5Cqquad%20%5Cqquad%20%5Cqquad%20b%5E*%3D%5Cbar%7By%7D-%5Cbar%7Bx%7Dm%5E*%20%5Ctag%7B11%7D)

以上就是一元线性回归的最小二乘法求解过程。


## 多元线性回归
现在我们把 x 扩展为多元的情况，即多种因素共同影响变量 y。现实问题也往往是这种情况，比如，要预测房价，需要考虑包括`是否学区`、`房间数量`、`周边是否繁华`、`交通方便性`等。共有 D 种维度的影响因素，机器学习领域将这 D 种影响因素称为特征（feature）。

比一元线性回归更为复杂的是，多元线性回归组成的不是直线，是一个多维空间中的超平面，数据点散落在超平面的两侧。

![](https://user-gold-cdn.xitu.io/2019/11/28/16eb0f6dd168e0ad?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

多元线性回归的损失函数仍然使用 `预测值-真实值` 的平方来计算，公式 16 为整个模型损失函数的向量表示。这里出现了一个竖线组成的部分，它被称作 L2 范数的平方。

![](https://juejin.im/equation?tex=L(w)%20%3D%20(%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D-%5Cmathbf%7By%7D)%5E%5Ctop%20(%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D-%5Cmathbf%7By%7D)%20%3D%20%7C%7C%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D-%5Cmathbf%7By%7D%7C%7C_2%5E2%20%5Ctag%7B16%7D)

线性规划可以得到一个确定的最优解，这些最优解组成了一个最优超平面。


## 优化方法

### 梯度下降
参考：[深入浅出--梯度下降法及其实现 - 简书](https://www.jianshu.com/p/c7e642877b0e)

#### 场景假设
>梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。

#### 梯度下降与下山
首先，我们有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向。所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。
   
为什么梯度的方向就是最陡峭的方向呢？
1. 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
2. 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向

#### 梯度下降的数学解释
![](https://upload-images.jianshu.io/upload_images/1234352-af8dd9722c762c13.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

解释：
1. α是什么含义？   
α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！

2. 为什么要梯度要乘以一个负号？   
梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号。


#### 梯度下降的简单实现
```py
import numpy as np

# Size of the points dataset.
m = 20

# Points x-coordinate and dummy value (x0, x1).
X0 = np.ones((m, 1))
X1 = np.arange(1, m+1).reshape(m, 1)
X = np.hstack((X0, X1))

# Points y-coordinate
y = np.array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m, 1)

# The Learning Rate alpha.
alpha = 0.01

def error_function(theta, X, y):
    """Error function J definition."""
    diff = np.dot(X, theta) - y
    return (1./2*m) * np.dot(np.transpose(diff), diff)

def gradient_function(theta, X, y):
    """Gradient of the function J definition."""
    diff = np.dot(X, theta) - y
    return (1./m) * np.dot(np.transpose(X), diff)

def gradient_descent(X, y, alpha):
    """Perform gradient descent."""
    theta = np.array([1, 1]).reshape(2, 1)
    gradient = gradient_function(theta, X, y)
    while not np.all(np.absolute(gradient) <= 1e-5):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, y)
    return theta

optimal = gradient_descent(X, y, alpha)
print('optimal:', optimal)
print('error function:', error_function(optimal, X, y)[0,0])
```

### 牛顿法
![](https://raw.githubusercontent.com/datawhalechina/team-learning/6fb859b4a6efbf0d1727683fbc5d2895482f4225/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/download2.png)
![](https://render.githubusercontent.com/render/math?math=f%28%5Ctheta%29%27%20%3D%20%5Cfrac%7Bf%28%5Ctheta%29%7D%7B%5CDelta%7D%2C%5CDelta%20%3D%20%5Ctheta_0%20-%20%5Ctheta_1&mode=display)
牛顿法的收敛速度非常快，但海森矩阵的计算较为复杂，尤其当参数的维度很多时，会耗费大量计算成本。我们可以用其他矩阵替代海森矩阵，用拟牛顿法进行估计。

### 拟牛顿法
拟牛顿法的思路是用一个矩阵替代计算复杂的海森矩阵H，因此要找到符合H性质的矩阵。


## 评价指标
参考：[06 模型之母：线性回归的评价指标 - 知乎](https://zhuanlan.zhihu.com/p/97426082)


## sklearn.linear_model
### 参数

1. `fit_intercept`: 默认为`True`，是否计算该模型的截距。如果使用中心化的数据，可以考虑设置为`False`，不考虑截距。注意这里是考虑，一般还是要考虑截距   

2. `normalize`: 默认为`False`。当`fit_intercept`设置为False的时候，这个参数会被自动忽略。如果为`True`,回归器会标准化输入参数：减去平均值，并且除以相应的二范数。当然啦，在这里还是建议将标准化的工作放在训练模型之前。通过设置`sklearn.preprocessing.StandardScaler`来实现，而在此处设置为`False`   

3. `copy_X`: 默认为`True`, 否则 X 会被改写   

4. `n_jobs`: 默认为 1，表示使用 CPU 的个数。当为 -1 时，代表使用全部 CPU。   

### 属性

1. `coef_`: 训练后的输入端模型系数，如果`label`有两个，即 y 值有两列。那么是一个 2D 的 array

2. `intercept_`: 截距

### methods:

1. `fit(X,y,sample_weight=None)`:    
`X`: array，稀疏矩阵 [n_samples,n_features]    
`y`: array，[n_samples, n_targets]    
`sample_weight`: array，权重。

2. `get_params(deep=True)`：返回对 regressor 的设置值

3. `predict(X)`: 预测 基于 R^2 值

4. `score`：评估


## 使用场景
1. 回归问题
2. 变量之间是线性关系
3. 误差服从均值为零的正太分布
4. 变量 x 的分布要有变异性
5. 多元线性回归不同特征之间相互独立
   
关于回归问题与分类问题：
>与回归相对的是分类问题（classification），分类问题要预测的变量 y 输出集合是有限的，预测值只能是有限集合内的一个。当要预测的变量 y 输出集合是无限且连续，我们称之为回归。比如，天气预报预测明天是否下雨，是一个二分类问题；预测明天的降雨量多少，就是一个回归问题。

## 代码实践
1. 调包
```py
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

np.random.seed(1234) #生成随机数

# 构建映射关系，模拟真实的数据待预测值
# 映射关系为 y = 4.2 + 5.7*x1 + 10.8*x2
# 可自行设置值进行尝试
x = np.random.rand(500,3)
y = x.dot(np.array([4.2,5.7,10.8]))


# 调用模型
lr = LinearRegression(fit_intercept=True)

# 训练模型
lr.fit(x,y)
print("估计的参数值为：%s" %(lr.coef_))

# 计算R平方
print('R2:%s' %(lr.score(x,y)))

# 任意设定变量，预测目标值
x_test = np.array([2,4,5]).reshape(1,-1)
y_hat = lr.predict(x_test)
print("预测值为: %s" %(y_hat))
```
2. 最小二乘法与梯度下降
```py
import numpy as np


class LinearModel(object):
    """求解斜率和截距"""
    def __init__(self):
        # 生成一个随机的斜率和截距
        self.w = np.random.randn(1)[0]
        self.b = np.random.randn(1)[0]
    
    def model(self, x):
        """方程
        f(x) =  wx + b
        """
        return self.w * x + self.b
    
    def loss(self, x, y):
        """损失
        线性问题，原理都是最小二乘法
        """
        model = self.model(x)
        
        # 损失，即最小二乘法
        # model 即方程
        # 方程中有几个未知数？
        cost = (y - model) ** 2
        
        # 求偏导数（导数是偏导数的一种特殊形式，即只有一个未知数时）
        # 偏导数 即把其它的都当成已知数，求一个未知数的导数
        # -x 即 y - model 的导数
        g_w = 2 * (y - model) * (-x)
        # -1 也是 y - model 的导数，但此时 w 已知，所以为 -1
        g_b = 2 * (y - model) * (-1)
        
        return g_w, g_b
    
    def gradient_descend(self, g_w, g_b, step=0.01):
        """梯度下降"""
        # 更新新的斜率和截距
        self.w = self.w - g_w * step
        self.b = self.b - g_b * step
        
    def fit(self, X, y):
        # 上一步的值
        w_last = self.w + 1
        b_last = self.b + 1
        
        # 精度
        precision = 0.00001
        
        # 最多执行次数
        max_count = 3000
        
        count = 0
        
        while True:
            # 满足精度 退出
            if (np.abs(self.w - w_last) < precision) and (np.abs(self.b - b_last) < precision):
                break
            
            # 达到最大次数 退出
            if count > max_count:
                break
            
            # 梯度
            g_w = 0
            g_b = 0
            
            # 长度
            size = X.shape[0]
            
            # 更新斜率和截距
            for i, j in zip(X, y):
                w, b = self.loss(i, j)
                # 求平均下降梯度
                g_w += w / size
                g_b += b / size
            
            # 更新
            self.gradient_descend(g_w, g_b)
            
            # 更新 count
            count += 1
            
    @property
    def coef_(self):
        return self.w
    
    @property
    def intercept_(self):
        return self.b
```


## 参考
1. [机器学习：一文读懂线性回归的数学原理](https://juejin.im/post/5ddf7a97f265da05e97b4f10)
2. [线性回归 - 维基百科，自由的百科全书](https://zh.wikipedia.org/zh-cn/%E7%B7%9A%E6%80%A7%E5%9B%9E%E6%AD%B8)
3. [斯坦福 吴恩达《CS229机器学习》_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili](https://www.bilibili.com/video/av29430384/)
4. [trans_datawhale/Task1_Linear_regression.ipynb at master · sarizzm/trans_datawhale](https://github.com/sarizzm/trans_datawhale/blob/master/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/Task1_Linear_regression.ipynb)
