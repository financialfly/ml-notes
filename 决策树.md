决策树
---------

- [决策树](#---)
- [前言](#--)
- [信息论基础](#-----)
- [算法](#--)
- [ID3](#id3)
  * [实例：SNS 虚假账户检测](#---sns-------)
- [C4.5](#c45)
  * [过拟合和剪枝](#------)
- [CART](#cart)
- [sklearn.DecisionTreeClassifier](#sklearndecisiontreeclassifier)
  * [参数](#--)
  * [使用](#--)
- [集成树](#---)
  * [套袋法 （bagging）](#-----bagging-)
  * [AdaBoost](#adaboost)
  * [梯度树提升 (Gradient Tree Boosting)](#-------gradient-tree-boosting-)
  * [XGBoost](#xgboost)
  * [LGBM (lightgbm)](#lgbm--lightgbm-)
- [team-learning 代码实现](#team-learning-----)
- [参考](#--)


## 前言
比如一句话可能会有很多意思（如：“喜欢上一个人”），有时候信息也是如此，信息如果有顺序排列，那么它的意思是明确的（意思少），如果是无序的，那么就会有很多意思。香农引入了物理学中的熵到信息学中，在计算机中描述信息的混乱度使用信息熵来表示。熵代表一个系统的混乱度。

## 信息论基础
首先先来几个概念,我们后面介绍决策树原理的时候会提到,这里可以先扫一眼,用到的时候再回来看.
1. 熵和信息熵

熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量. 可以看出,熵表示的是体系的不确定性大小. 熵越大, 物理的不确定性越大. 1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题. 同理, 信息熵越小,数据的稳定性越好,我们更加相信此时数据得到的结论. 换言之, 我们现在目的肯定熵越小,机器学习得到的结果越准确. 

信息熵表示随机变量不确定性的度量,设随机标量X是一个离散随机变量，其概率分布为:
$$P(X=x_i)=p_i, i=1,2,...,n$$
则随机变量X的熵定义为:
$$H(X)=-\sum_{i=1}^{n}p_ilog{p_i}$$
熵越大，随机变量的不确定性就越大，当$p_i=\frac{1}{n}$时，
随机变量的熵最大等于logn，故$0 \leq H(P) \leq logn$.

2. 条件熵

条件熵就是在给定X的条件的情况下，随机标量Y的条件，记作$H(Y|X)$，可以结合贝叶斯公式进行理解，定义如下
$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$
这里$p_i=P(X=x_i),i=1,2,...,n$.
一般在基于数据的估计中，我们使用的基于极大似然估计出来的经验熵和经验条件熵.

3. 联合熵

联合熵是相对两个及其以上的变量而言的, 两个变量X和Y的联合信息熵为:

$$ H(X,Y)=-\sum_x \sum_y P(x,y)log_2[P(x,y)] $$

其中: x和y是X和Y的特定值, 相应地, 是这些值一起出现的联合概率, 若为0, 则定义为0.

对于两个以上的变量$X_1,...,X_n$,一般形式位:
$$H(X_1,...,X_n)=-\sum_{x_1}\cdot \cdot \cdot\sum_{x_n}P(x1,...,x_n)log_2[P(x_1,...,x_n)]$$

性质:
- 大于每个独立的熵
$$H(X,Y) \geq max(H(X),H(Y))$$
$$H(X_1,...,X_n) \geq max(H(X_1),...,H(X_n))$$
- 小于独立熵的和
$$H(X_1,...,X_n) \leq sum(H(X_1),...,H(X_n))$$
- 和条件熵的关系
$$H(Y|X)=H(X,Y)-H(X)$$
- 和互信息的关系
$$I(Y;X)=H(X)+H(Y)-H(X,Y)=H(Y)-(H(X,Y)-H(X))$$


4. 信息增益

信息增益又叫**互信息**,它表示的是在的得知特征X的信息后,使得类Y的信息的不确定性(熵)减少的程度. 
$$g(Y,X)=H(Y)-H(Y|X)$$


5. 基尼指数

基尼指数又称基尼系数或者基尼不纯度,基尼系数是指国际上通用的、用以衡量一个国家或地区居民收入差距的常用指标. 在信息学中,例如分类问题, 假设有K个类,样本点属于第k类的概率是$p_k$,则该概率分布的基尼指数定义为:
$$Gini(p)=\sum_k^Kp_k(1-p_k)=1-\sum_k^Kp_k^2$$

基尼指数和熵一样，同样表示集合D的不确定性，基尼指数(Gini(D,A))表示根据调教A=a后的集合D的不确定性，基尼指数越大，表示数据D的不确定性越大。


## 算法
所有种类的决策树算法有哪些以及它们之间的区别？

1. ID3（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。

2. C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。

3. C5.0 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。

4. CART（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。


## ID3
划分原则：将无序的数据变得更加有序。

计算熵：

$$
H=-\sum_{i=1}^{n} p\left(x_{i}\right) \log _{2} p\left(x_{i}\right)
$$

其中：
    
    p 代表概率
    n 是分类的数目
    前面加负号的原因是为了让结果变得有意义（后面的计算结果为一个负数）
    
    
在决策树当中，设 D 为用类别对训练元组进行的划分，则 D 的熵为：

$$
lnfo(D)=-\sum_{i=1}^{m} p_{i} \log _{2}\left(p_{i}\right)
$$

将 D 按属性 A 进行划分，则 A 对 D 划分的期望信息为：

$$
lnfo_{A}(D)=\sum_{j=1}^{v} \frac{\left|D_{j}\right|}{|D|} \times \operatorname{Info}\left(D_{j}\right)
$$

D 的熵与 A 对 D 划分的期望信息两者的差值即为信息增益：

$$
\operatorname{gain}(A)=\operatorname{info}(D)-i n f o_{A}(D)
$$

一个系统的信息熵可以超过 1，其最小值为 0，表示该系统极为稳定。计算为 `1 * np.log2(1)`

### 实例：SNS 虚假账户检测
假设训练集合包含 10 个元素：

<img src="https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/linerregression/SNS.png" width="50%" />

其中 s，m，l 分别表示小、中、大。

在训练数据中，账号是否真实即对应公式中的 D，而前面的三个条件可以当作是 A, B, C。

在 D 中，no 的占比为 0.3
```py
import numpy as np

info_D = -(0.3 * np.log2(0.3) + 0.7 * np.log2(0.7))
# info_D: 0.8812908992306927

# 按照日志密度进行划分，效果要相对好一些（因为熵减小了，说明系统有序了）
# 信息增益为：
info_D - info_A_D
# output: 0.2812908992306927

# 按好友密度进行划分
#     s 0.4 -> 3 no + 1 yes
#     m 0.4 -> 4 yes
#     l 0.2 -> 2 yes
info_B_D = -0.4*(3/4*np.log2(3/4) + 1/4*np.log2(1/4)) - 0.4*1*np.log2(1) - 0.2*1*np.log2(1)
# info_B_D: 0.32451124978365314

# 信息增益为 相对于日志密度较大
info_D - info_B_D
# output: 0.5567796494470396
```

## C4.5
C4.5 按照百分比计算信息增益。
```py
# 按照 C4.5，在 ID3 中的例子中，信息增益的计算可以为：

(info_D - info_A_D) / info_A_D
# output: 0.4688181653844879

(info_D - info_B_D) / info_B_D
# output: 1.7157483748814144
```

### 过拟合和剪枝

决策树建立的过程中,只考虑经验损失最小化,没有考虑结构损失. 因此可能在训练集上表现良好,但是会出现过拟合问题.(我们构造决策树的时候，是完全的在训练数据上得到最优的模型. 这就是过拟合问题，训练误差很小，但是验证集上就不怎么好用.) 为了解决过拟合,我们从模型损失进行考虑:

$$模型损失=经验风险最小化+正则项=结构风险最小化$$

思路很简单,给损失函数加上正则项再进行优化. 正则项表示树节点的个数,因此有如下公式:

$$C_{\alpha}(T)=C(T)+\alpha|T|$$

进一步详细定义,解决问题:


重新定义损失函数，树的叶子节点个数|T|,t是树T的叶节点，该叶节点有$N_t$个样本，其中k类的样本点有$N_{tk}$个，k=1,2，...,K, $H_t(T)$是叶子节点t经验熵，$\alpha \leq 0$是参数,平衡经验损失和正则项，得到计算公式如下:
$$C_{\alpha}(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$
其中，经验熵为:
$$H_t(T)=-\sum_{k}\frac{N_{tk}}{H_t}log\frac{N_{tk}}{H_t}$$
这是有:
$$C_{\alpha}=C(T)+\alpha|T|$$
决策树剪枝优化过程考虑了在训练数据上的经验风险最小化和减小模型复杂度两个方向. 因为加了正则项，所有我们基于贪心的思想进行剪枝，因为当剪掉一个树节点，虽然经验风险增加了，但是模型复杂度降低了，我们基于两个方面的平衡进行剪枝，如果剪掉之后，总的风险变小，就进行剪枝.       
算法:       
输入: 算法产生的整个决策树，参数$\alpha$        
修剪之后的树$T_{\alpha}$        
1. 计算每个节点的经验熵
2. 递归从树的叶节点向上回溯，假设将某个叶节点回缩到其父节点前后的整体树对应的$T_B$和$T_A$,对应的损失分别是$C_{\alpha}(T_B)$和$C_{\alpha}(T_A)$，如果:
   $$C_{\alpha}(T_A) \leq C_{\alpha}(T_B)$$
    表示，剪掉之后，损失减小，就进行剪枝.
3. 重复2，直到不能继续为止，得到损失函数最小的子树$T_{\alpha}$. 


## CART
参考：[机器学习之手把手实现，第 5 部分: 机器学习系列之手把手教你实现一个分类回归树](https://www.ibm.com/developerworks/cn/analytics/library/machine-learning-hands-on5-cart-tree/index.html)



## sklearn.DecisionTreeClassifier
### 参数
```
criterion : string, optional (default="gini")
    The function to measure the quality of a split. Supported criteria are
    "gini" for the Gini impurity and "entropy" for the information gain.
    
    entropy 即为熵。

splitter : string, optional (default="best")
    The strategy used to choose the split at each node. Supported
    strategies are "best" to choose the best split and "random" to choose
    the best random split.
    
    最优策略会选择信息增益最大的

max_depth : int or None, optional (default=None)
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.
    
    默认最大深度

min_samples_split : int, float, optional (default=2)
    The minimum number of samples required to split an internal node:

    - If int, then consider `min_samples_split` as the minimum number.
    - If float, then `min_samples_split` is a fraction and
      `ceil(min_samples_split * n_samples)` are the minimum
      number of samples for each split.

    .. versionchanged:: 0.18
       Added float values for fractions.
       
    默认构建标准的二叉树

min_samples_leaf : int, float, optional (default=1)
    The minimum number of samples required to be at a leaf node.
    A split point at any depth will only be considered if it leaves at
    least ``min_samples_leaf`` training samples in each of the left and
    right branches.  This may have the effect of smoothing the model,
    especially in regression.

    - If int, then consider `min_samples_leaf` as the minimum number.
    - If float, then `min_samples_leaf` is a fraction and
      `ceil(min_samples_leaf * n_samples)` are the minimum
      number of samples for each node.

    .. versionchanged:: 0.18
       Added float values for fractions.

min_weight_fraction_leaf : float, optional (default=0.)
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.

max_features : int, float, string or None, optional (default=None)
    The number of features to consider when looking for the best split:

        - If int, then consider `max_features` features at each split.
        - If float, then `max_features` is a fraction and
          `int(max_features * n_features)` features are considered at each
          split.
        - If "auto", then `max_features=sqrt(n_features)`.
        - If "sqrt", then `max_features=sqrt(n_features)`.
        - If "log2", then `max_features=log2(n_features)`.
        - If None, then `max_features=n_features`.

    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than ``max_features`` features.

random_state : int, RandomState instance or None, optional (default=None)
    If int, random_state is the seed used by the random number generator;
    If RandomState instance, random_state is the random number generator;
    If None, the random number generator is the RandomState instance used
    by `np.random`.

max_leaf_nodes : int or None, optional (default=None)
    Grow a tree with ``max_leaf_nodes`` in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.

min_impurity_decrease : float, optional (default=0.)
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.

    The weighted impurity decrease equation is the following::

        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)

    where ``N`` is the total number of samples, ``N_t`` is the number of
    samples at the current node, ``N_t_L`` is the number of samples in the
    left child, and ``N_t_R`` is the number of samples in the right child.

    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
    if ``sample_weight`` is passed.

    .. versionadded:: 0.19

min_impurity_split : float, (default=1e-7)
    Threshold for early stopping in tree growth. A node will split
    if its impurity is above the threshold, otherwise it is a leaf.

    .. deprecated:: 0.19
       ``min_impurity_split`` has been deprecated in favor of
       ``min_impurity_decrease`` in 0.19. The default value of
       ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
       will be removed in 0.25. Use ``min_impurity_decrease`` instead.

class_weight : dict, list of dicts, "balanced" or None, default=None
    Weights associated with classes in the form ``{class_label: weight}``.
    If not given, all classes are supposed to have weight one. For
    multi-output problems, a list of dicts can be provided in the same
    order as the columns of y.

    Note that for multioutput (including multilabel) weights should be
    defined for each class of every column in its own dict. For example,
    for four-class multilabel classification weights should be
    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
    [{1:1}, {2:5}, {3:1}, {4:1}].

    The "balanced" mode uses the values of y to automatically adjust
    weights inversely proportional to class frequencies in the input data
    as ``n_samples / (n_classes * np.bincount(y))``

    For multi-output, the weights of each column of y will be multiplied.

    Note that these weights will be multiplied with sample_weight (passed
    through the fit method) if sample_weight is specified.

presort : bool, optional (default=False)
    Whether to presort the data to speed up the finding of best splits in
    fitting. For the default settings of a decision tree on large
    datasets, setting this to true may slow down the training process.
    When using either a smaller dataset or a restricted depth, this may
    speed up the training.

Attributes
----------
classes_ : array of shape = [n_classes] or a list of such arrays
    The classes labels (single output problem),
    or a list of arrays of class labels (multi-output problem).

feature_importances_ : array of shape = [n_features]
    The feature importances. The higher, the more important the
    feature. The importance of a feature is computed as the (normalized)
    total reduction of the criterion brought by that feature.  It is also
    known as the Gini importance [4]_.

max_features_ : int,
    The inferred value of max_features.

n_classes_ : int or list
    The number of classes (for single output problems),
    or a list containing the number of classes for each
    output (for multi-output problems).

n_features_ : int
    The number of features when ``fit`` is performed.

n_outputs_ : int
    The number of outputs when ``fit`` is performed.

tree_ : Tree object
    The underlying Tree object. Please refer to
    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and
    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`
    for basic usage of these attributes.
```

### 使用
```py
import graphviz
import numpy as np

import sklearn.datasets as datasets

from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import train_test_split

# 使用自带的 iris 数据集
iris = datasets.load_iris()

X = iris['data']
y = iris['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# classifier
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

clf.score(X_test, y_test)

# 决策树为树结构，可以画图展示
dot_data = export_graphviz(clf, out_file=None, 
                           feature_names=iris.feature_names, 
                           class_names=iris.target_names, 
                           filled=True)


graph = graphviz.Source(dot_data)

# 存储到 iris.pdf (默认为 pdf 格式)
graph.render('./iris')
```
![](https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/d-tree/iris-decesion-tree.png)


## 集成树
由于决策树算法的改进，目前实际工作中很少会用到决策树了，而常用的是集成树（集成算法）。

### 套袋法 （bagging）
在集成算法中，bagging 方法会在原始训练集的随机子集上构建一类黑盒估计器的多个实例，然后把这些估计器的预测结果结合起来形成最终的预测结果。

即：给多棵树，每棵树都是相似的，但数据不同，每棵树就像袋子一样将数据封装起来。

1. 随机森林   
随机森林包含多颗决策树，每颗决策树都是通过对样本进行随机抽样得来的，最后通过投票得到最终预测结果。一般情况下，随机森林准确度要比决策树的准确度要高。
```
    X_train -> 随机抽样，选取子集 -> X_train1, X_train2, ... X_trainn
    
    裂分，找每个子集的最佳属性
    
    ...
    
    预测新数据时，通过多棵树对目标进行预测，根据各棵树预测的结果选取概率最大的结果。
```
```py
from sklearn.ensemble import RandomForestClassifier

forest = RandomForestClassifier(n_estimators=100)

forest.fit(X_train, y_train)

forest.score(X_test, y_test)
```

2. 极限随机森林   
在极限随机树中，计算分割点方法中的随机性进一步增强。与随机森林相同，使用的特征是候选特征的随机子集；但是不同于随机森林寻找最具有区分度的阈值，这里的阈值是针对每个候选特征随机生成的，并且选择这些随机生成的阈值中的最佳者作为分割规则。 这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差。
```py
from sklearn.ensemble import ExtraTreesClassifier

extra_tree = ExtraTreesClassifier(n_estimators=100)

extra_tree.fit(X_train, y_train)

extra_tree.score(X_test, y_test)

dot_data = export_graphviz(extra_tree.estimators_[0], 
                           out_file=None, 
                           feature_names=iris.feature_names, 
                           filled=True, 
                           class_names=iris.target_names)
graph = graphviz.Source(dot_data)
graph.render('./iris-extra-tree')
```

### AdaBoost
AdaBoost 的核心思想是用反复修改的数据（主要是修正数据的权重）来训练一系列的弱学习器(一个弱学习器模型仅仅比随机猜测好一点, 比如一个简单的决策树),由这些弱学习器的预测结果通过加权投票(或加权求和)的方式组合, 得到我们最终的预测结果。示例图：

<img src="https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/d-tree/adaboost.png" width="80%" />

一个数据之所以没有区分开，是因为它的特征还不够明显。因此 adaboost 可以将数据放大，系数放大，就像拿一个放大镜看数据一样。adaboost 的每个子树深度都为 2。
```py
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris
from sklearn.ensemble import AdaBoostClassifier

iris = load_iris()

# 参数 learning_rate 即学习速率，也叫步幅
clf = AdaBoostClassifier(n_estimators=100)

# 交叉验证
# 会将数据分为 3 份，可通过调整参数 cv 来指定划分份数
scores = cross_val_score(clf, iris.data, iris.target)
scores.mean()
```

### 梯度树提升 (Gradient Tree Boosting)
Gradient Tree Boosting 或梯度提升回归树（GBRT）是对于任意的可微损失函数的提升算法的泛化。 GBRT 是一个准确高效的现有程序， 它既能用于分类问题也可以用于回归问题。梯度树提升模型被应用到各种领域，包括网页搜索排名和生态领域。
   
<center>
<img src="https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/d-tree/Gradient-Tree-Boosting.png" width="80%">
梯度树提升示例图
</center>

GBRT 的优点:

    对混合型数据的自然处理（异构特征）
    强大的预测能力
    在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)

GBRT 的缺点:

    可扩展性差（此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，
    而非我们通常说的功能的扩展性；GBRT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。
    由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行。

```py
from pandas import Series, DataFrame
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

X = DataFrame({'购物': [500, 500, 1500, 1500], '是否百度问答': [0, 1, 0, 1]})
y = np.array([14, 16, 24, 26])

# 回归问题 所以使用 GradientBoostingRegressor
gbdt = GradientBoostingRegressor(n_estimators=10)
gbdt.fit(X, y)

gbdt.score(X, y)

dot = export_graphviz(gbdt.estimators_[0, 0], out_file=None, feature_names=X.columns, filled=True)

graph = graphviz.Source(dot)
graph.render('./gdbt-test')

# 图解释
# 梯度前
((y - y.mean()) ** 2).mean()
# output: 26.0

# 第一次梯度
y_ = gbdt.estimators_[1, 0].predict(X)
((y_ - y_.mean()) ** 2).mean()
# output: 21.06

# 第二次梯度
y_
# output: array([-5.4, -3.6,  3.6,  5.4])
```

### XGBoost
XGBoost 是对梯度提升树的改进和升级。
参考：[XGBoost Documentation — xgboost 1.0.0-SNAPSHOT documentation](https://xgboost.readthedocs.io/en/latest/)

```py
from xgboost import XGBClassifier

from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split

# 红酒数据，分类问题

wine = load_wine()

X = wine['data']
y = wine['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

xgboost = XGBClassifier()

xgboost.fit(X_train, y_train)

xgboost.score(X_test, y_test)
```

### LGBM (lightgbm)
参考：[Welcome to LightGBM’s documentation! — LightGBM 2.3.2 documentation](https://lightgbm.readthedocs.io/en/latest/)
```py
from lightgbm import LGBMClassifier

clf = LGBMClassifier()

clf.fit(X_train, y_train)

clf.score(X_test, y_test)
```

## team-learning 代码实现
[ml-notes/decision-tree.py](https://github.com/financialfly/ml-notes/blob/master/tl-code/decision-tree.py)


## 参考
1. [1.11. 集成方法 · sklearn 中文文档](https://sklearn.apachecn.org/docs/0.21.3/12.html)
2. [team-learning/Task4_decision_tree.ipynb at master · datawhalechina/team-learning](https://github.com/datawhalechina/team-learning/blob/master/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/Task4_decision_tree.ipynb)
