聚类
----

## 目录
- [聚类与无监督学习](#聚类与无监督学习)
- [性能度量](#性能度量)
- [距离计算](#距离计算)
- [原型聚类](#原型聚类)
  * [K-Means](#k-means)
    + [步骤](#步骤)
    + [team-learning 代码实现](#team-learning-代码实现)
    + [K](#k)
    + [轮廓系数](#轮廓系数)
  * [LVQ](#lvq)
  * [高斯混合聚类](#高斯混合聚类)
- [层次聚类](#层次聚类)
  * [AGNES](#agnes)
  * [自顶向下](#自顶向下)
- [密度聚类](#密度聚类)
  * [DBSCAN](#dbscan)
- [聚类方法对比](#聚类方法对比)
- [sklearn](#sklearn)
- [参考](#参考)


## 聚类与无监督学习
无监督学习是机器学习的一种方法，没有给定事先标记过的训练示例，自动对输入的数据进行分类或分群。无监督学习的主要运用包含：聚类分析、关系规则、维度缩减。它是监督式学习和强化学习等策略之外的一种选择。 一个常见的无监督学习是数据聚类。在人工神经网络中，生成对抗网络、自组织映射和适应性共振理论则是最常用的非监督式学习。

聚类是一种无监督学习。聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集，这样让在同一个子集中的成员对象都有相似的一些属性，常见的包括在坐标系中更加短的空间距离等。


## 性能度量
在机器学习中我们都需要对任务进行评价以便于进行下一步的优化，聚类的性能度量主要有一下两种。
1. 外部指标：是指把算法得到的划分结果跟某个外部的“参考模型”（如专家给出的划分结果）比较
2. 内部指标：是指直接考察聚类结果，不利用任何参考模型的指标。


## 距离计算
在机器学习和数据挖掘中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。常见距离计算方式有：

* 欧式距离（2-norm 距离）
* 曼哈顿距离（Manhattan distance, 1-norm 距离）
* 切比雪夫距离
* 闵可夫斯基距离
* 余弦相似性
* 马氏距离

### 欧式距离   
欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。
$$d(x,y)=\sqrt{\Sigma_{k=1}^n (x_k-y_k)^2}$$

### 切比雪夫距离    
$$d(x,y) = \lim_{n\rightarrow \infty} (\Sigma_{k=1}^n (\left|x_k-y_k\right|)^r)^\dfrac{1}{r} = max_k (\left|x_k-y_k\right|)$$

### 闵可夫斯基距离   
$$d(x,y)=(\Sigma_{k=1}^n (\left|x_k-y_k\right|)^r)^\dfrac{1}{r}$$

式中，r 是一个可变参数，根据参数 r 取值的不同，闵可夫斯基距离可以表示一类距离  
  r = 1 时，为曼哈顿距离  
  r = 2 时，为欧式距离  
  r → ∞ 时，为切比雪夫距离  
闵可夫斯基距离包括欧式距离、曼哈顿距离、切比雪夫距离都假设数据各维属性的量纲和分布（期望、方差）相同，
因此适用于度量独立同分布的数据对象。

### 余弦距离      
余弦相似度公式定义如下：

$$cos⁡(x,y)=\dfrac{xy}{\left|x\right|\left|y\right|} = \dfrac{\Sigma_{k=1}^n x_k y_k}{\sqrt{\Sigma_{k=1}^n x_k^2} \sqrt{\Sigma_{k=1}^n y_k^2}}$$

余弦相似度实际上是向量 xx 和 yy 夹角的余弦度量，可用来衡量两个向量方向的差异。
如果余弦相似度为 11，则 xx 和 yy 之间夹角为 0°0°，两向量除模外可认为是相同的；
如果预先相似度为 00，则 xx 和 yy 之间夹角为 90°90°，则认为两向量完全不同。
在计算余弦距离时，将向量均规范化成具有长度 11，因此不用考虑两个数据对象的量值。
余弦相似度常用来度量文本之间的相似性。
文档可以用向量表示，向量的每个属性代表一个特定的词或术语在文档中出现的频率，尽管文档具有大量的属性，
但每个文档向量都是稀疏的，具有相对较少的非零属性值。

### 马氏距离   
$$mahalanobis(x,y)=(x-y)\Sigma^{-1}(x-y)^T$$
    
式中，Σ−1Σ−1 是数据协方差矩阵的逆。
前面的距离度量方法大都假设样本独立同分布、数据属性之间不相关。马氏距离考虑了数据属性之间的相关性，排除了属性间相关性的干扰，而且与量纲无关。若协方差矩阵是对角阵，则马氏距离变成了标准欧式距离；若协方差矩阵是单位矩阵，各个样本向量之间独立同分布，则变成欧式距离。


## 原型聚类
原型聚类亦称"基于原型的聚类" (prototype-based clustering)，此类算法假设聚类结构能通过一组原型刻画，在现实聚类任务中极为常用.通常情形下，算法先对原型进行初始化，然后对原型进行迭代更新求解.采用不同的原型表示、不同的求解方式，将产生不同的算法。

* K 均值
* LVQ
* 高斯混合聚类

### K-Means
K-Means 算法（也叫 K 均值算法）是一种据类分析的算法，其主要是来计算数据集的算法，主要通过不断地取离种子点最近均值的算法，K-Means 算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为 K 个簇，让簇内的点尽量紧密的连在一起，而让簇键的距离尽量的大。
K-Means 使用欧氏距离计算各个元素之间的距离。

#### 步骤
1. 从数据中选择 k 个对象作为初始聚类中心
2. 计算每个聚类对象到聚类中心的距离来划分
3. 再次计算每个聚类中心
4. 计算标准测度函数，直到达到最大迭代次数，则停止，否则继续操作
5. 确定最优的聚类中心

#### team-learning 代码实现
```py
def distEclud(vecA, vecB):
    '''
    欧氏距离计算函数
    :param vecA:
    :param vecB:
    
    :return: float 
    '''
    dist = 0.0
    # ========= show me your code ==================
    dist = np.linalg.norm((vecA-vecB))
    # here
    # ========= show me your code ==================
    return dist


def randCent(dataMat, k):
    '''
    为给定数据集构建一个包含K个随机质心的集合,
    随机质心必须要在整个数据集的边界之内,这可以通过找到数据集每一维的最小和最大值来完成
    然后生成0到1.0之间的随机数并通过取值范围和最小值,以便确保随机点在数据的边界之内
    :param np.dataMat:
    :param k:
    
    :return: np.dataMat
    '''
    # 获取样本数与特征值
    m, n = np.shape(dataMat)
    # 初始化质心,创建(k,n)个以零填充的矩阵
    centroids = np.mat(np.zeros((k, n)))
    print(centroids)
    
    # ========= show me your code ==================
    # 循环遍历特征值
    # here
    for j in range(n):
    
        #获得第j 列的最小值
        minj = np.min(dataMat[:,j]) 

        #得到最大值与最小值之间的范围
        rangej = float(np.max(dataMat[:,j]) - minj)     
        
        #获得输出为 K 行 1 列的数据，并且使其在数据集范围内
        centroids[:,j] = np.mat(minj + rangej * np.random.rand(k, 1))   
    
    # ========= show me your code ==================
    
    # 返回质心
    return centroids.A


def kMeans(dataMat, k, distMeas=distEclud):
    '''
    创建K个质心,然后将每个店分配到最近的质心,再重新计算质心。
    这个过程重复数次,直到数据点的簇分配结果不再改变为止
    :param dataMat: 数据集
    :param k: 簇的数目
    :param distMeans: 计算距离
    :return:
    '''
    # 获取样本数和特征数
    m, n = np.shape(dataMat)
    # 初始化一个矩阵来存储每个点的簇分配结果
    # clusterAssment包含两个列:
    # 一列记录簇索引值,
    # 第二列存储误差(误差是指当前点到簇质心的距离,后面会使用该误差来评价聚类的效果)
    clusterAssment = np.mat(np.zeros((m, 2)))
    # 创建质心,随机K个质心
    centroids = randCent(dataMat, k)

    # 初始化标志变量,用于判断迭代是否继续,如果True,则继续迭代
    clusterChanged = True
    while clusterChanged:
        clusterChanged = False
        # 遍历所有数据找到距离每个点最近的质心,
        # 可以通过对每个点遍历所有质心并计算点到每个质心的距离来完成
        for i in range(m):
            minDist = float("inf")
            minIndex = -1
            for j in range(k):
                # 计算数据点到质心的距离
                # 计算距离是使用distMeas参数给出的距离公式,默认距离函数是distEclud
                distJI = distMeas(centroids[j, :], dataMat[i, :])
                # 如果距离比minDist(最小距离)还小,更新minDist(最小距离)和最小质心的index(索引)
                if distJI < minDist:
                    minDist = distJI
                    minIndex = j
            
            # 如果任一点的簇分配结果发生改变,则更新clusterChanged标志
            # ========= show me your code ==================
            # here
            if clusterAssment[i, 0] != minIndex: 
                clusterChanged = True
            # ========= show me your code ==================
            
            # 更新簇分配结果为最小质心的index(索引),minDist(最小距离)的平方
            clusterAssment[i, :] = minIndex, minDist ** 2
        # print(centroids)
        
        
        # 遍历所有质心并更新它们的取值
        # ========= show me your code ==================
        for cent in range(k):
            
            #分别找到属于k类的数据
            ptsInClust = dataMat[np.nonzero(clusterAssment[:,0].A == cent)[0]]

            #得到更新后的中心点 
            centroids[cent,:] = np.mean(ptsInClust, axis = 0)   
        # here
        # ========= show me your code ==================
    
    # 返回所有的类质心与点分配结果
    return centroids, clusterAssment


# 运行Kmeans，假设有两聚类中心
# center,label_pred = kMeans(X, k=2)

# 将标签转化成易绘图的形式
# label = label_pred[:, 0].A.reshape(-1)

# 将结果可视化
# plt.scatter(X[:, 0], X[:, 1], c=label)
# plt.scatter(center[0, 0], center[0, 1], marker="*", s = 100)
# plt.scatter(center[1, 0], center[1, 1], marker="*", s = 100)
# plt.show()
```

#### K
```py
import matplotlib.pyplot as plt
import sklearn.datasets as datasets

X, y = datasets.make_blobs()

plt.scatter(X[:, 0], X[:, 1], c=y)
```

#### 轮廓系数
```py
# 轮廓系数（评价指标）
from sklearn.metrics import silhouette_score

kmeans = KMeans(2)

# X, y = datasets.make_blobs()
kmeans.fit(X)

y_ = kmeans.predict(X)

plt.scatter(X[:,0], X[:,1], c=y_)

# 评价
# The Silhouette Coefficient is calculated using the mean intra-cluster （簇内的点）
# distance (``a``) and the mean nearest-cluster distance (``b``) for each
# sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
# b)``.  To clarify, ``b`` is the distance between a sample and the nearest
# cluster that the sample is not a part of.
silhouette_score(X, y_)
```


### LVQ
学习向量量化(Learning Vector Quantization,简称 LVQ)属于原型聚类，即试图找到一组原型向量来聚类，每个原型向量代表一个簇，将空间划分为若干个簇，从而对于任意的样本，可以将它划入到它距离最近的簇中，不同的是 LVQ 假设数据样本带有类别标记，因此可以利用这些类别标记来辅助聚类。
大致思想如下：

1. 统计样本的类别，假设一共有 q 类，初始化为原型向量的标记为 {t1,t2,……,tq}。从样本中随机选取 q 个样本点位原型向量 {p1, p2 ,……, pq}。初始化一个学习率 a, a 取值范围(0,1)。
2. 从样本集中随机选取一个样本(x, y)，计算该样本与 q 个原型向量的距离（欧几里得距离），找到最小的那个原型向量 p，判断样本的标记 y 与原型向量的标记 t 是不是一致。若一致则更新为 p’ = p + a*(x-p)，否则更新为 p’ = p – a*(x – p)。
3. 重复第 2 步直到满足停止条件。（如达到最大迭代次数）
4. 返回 q 个原型向量。

### 高斯混合聚类
高斯混合聚类与 k 均值、LVQ 用原型向量来刻画聚类结构不同，高斯混合聚类采用概率模型来表达聚类原型。相对于 k 均值聚类算法使用 k 个原型向量来表达聚类结构，高斯混合聚类使用 k 个高斯概率密度函数混合来表达聚类结构

$$P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )$$

于是迭代更新 k 个簇原型向量的工作转换为了迭代更新 k 个高斯概率密度函数的任务。每个高斯概率密度函数代表一个簇，当一个新的样本进来时，我们可以通过这 k 的函数的值来为新样本分类。

高斯混合模型聚类算法 EM 步骤如下：

1. 猜测有几个类别，既有几个高斯分布。
2. 针对每一个高斯分布，随机给其均值和方差进行赋值。
3. 针对每一个样本，计算其在各个高斯分布下的概率。
4. 针对每一个高斯分布，每一个样本对该高斯分布的贡献可以由其下的概率表示，如概率大则表示贡献大，反之亦然。这样把样本对该高斯分布的贡献作为权重来计算加权的均值和方差。之后替代其原本的均值和方差。
5. 重复 3~4 直到每一个高斯分布的均值和方差收敛。
   
![](https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/cluster/gaussian-mixture.png)


## 层次聚类
层次聚类(hierarchical clustering)基于簇间的相似度在不同层次上分析数据，从而形成树形的聚类结构，层次聚类一般有两种划分策略：自底向上的聚合（agglomerative）策略和自顶向下的分拆（divisive）策略

* AGNES
* 自顶而下


### AGNES
AGNES算法是自底向上的层次聚类算法。开始时将数据集中的每个样本初始化为一个簇，然后找到距离最近的两个簇，将他们合并，不断重复这个过程，直达到到预设的聚类数目为止。

簇间距离的计算可以有三种形式：  
最小距离：$d_{min}(C_i,C_j)=\min_{p\in C_i,q\in C_j}|p-q|.$  
最大距离：$d_{max}(C_i,C_j)=\max_{p\in C_i,q\in C_j}|p-q|.$  
平均距离：$d_{avg}(C_i,C_j)=\frac{1}{|C_i||C_j|}\sum_{p\in C_i}\sum_{q\in C_j}|p-q|.$  

```
输入：样本集 D={x1,x2,...,xm}D={x1,x2,...,xm}
   聚类簇距离度量函数 dd；
   聚类簇数 kk

过程：

for j=1,2,...,mj=1,2,...,m do
    Cj={xj}Cj={xj}
end for

for i=1,2,...,mi=1,2,...,m do
    for i=1,2,...,mi=1,2,...,m do
        M(i,j)=d(Ci,Cj)M(i,j)=d(Ci,Cj);
        M(j,i)=M(i,j)M(j,i)=M(i,j);
    end for
end for

设置当前聚类簇个数：q=mq=m;

while q>kq>k do
    
    找出距离最近的两个聚类簇 Ci∗Ci∗ 和 Cj∗Cj∗;
    合并 Ci∗Ci∗ 和 Cj∗Cj∗：Ci∗=Ci∗⋃Cj∗Ci∗=Ci∗⋃Cj∗；
    
    for j=j∗+1,j∗+2,..,qj=j∗+1,j∗+2,..,q do
        将聚类簇 CjCj 重新编号为 CjCj
    end for
 
    删除距离矩阵 MM 的第 j∗j∗ 行和第 j∗j∗ 列;
    
    for j=1,2,...,q−1j=1,2,...,q−1 do
        M(i,j)=d(Ci,Cj)M(i,j)=d(Ci,Cj);
        M(j,i)=M(i,j)M(j,i)=M(i,j);
    end for
 
    q=q−1q=q−1

end while

输出：簇划分: C={C1,C2,...,Ck}
```

### 自顶向下
把整个数据集视作一个簇，然后把一个簇分成几个簇，接着再分别把每一个簇分成更小的簇，如此反复下去，直到满足要求为止。

## 密度聚类
密度聚类假设聚类结构通过样本分布的紧密程度。此算法是基于密度的角度来考察样本之间的连接性，并基于连接性不断扩展聚类簇最后获得最终的结果。通过判断样本在区域空间内是否大于某个阈值来决定是否将其放到与之相近的样本中。

* DBSCAN
* 其他密度聚类算法

### DBSCAN
e- 邻域：

    对 xj∈D, 其 ∈ 邻域包含样本集 D 中与 xj 的距离不大于 e 的样本，即 N(xj)= {xi∈D | dist(xi,xj)≤e}

核心对象(core object)：

    若 xj 的 E- 邻域至少包含 MinPts 个样本，即 |Ne(xj)|≥MinPts，则 xj 是一个核心对象

密度直达(directly density- reachable)：
    
    若 xj 位于 xi 的 e- 邻域中，且 xi 是核心对象,则称 x；由 xi 密度直达

密度可达(density. reachable)：
    
    对 xi 与 xj，若存在样本序列 P1,P2,...,Pn，其中 p1=xi，Pn=xj 且 pi+1 由 pi 密度直达，则称 xj 由 xi 密度可达 

密度相连(density-conected)：

    对 xi 与 xj，若存在 xk 使得 xi 与 xj 均由 xk 密度可达，则称 xi 与 xj 密度相连

```
首先将数据集 D 中的所有对象标记为未处理状态  

for（数据集 D 中每个对象 p） do  

    if （p 已经归入某个簇或标记为噪声） then  
         continue;  

    else  
         检查对象 p 的 Eps 邻域 NEps(p)；  
         if (NEps(p) 包含的对象数小于 MinPts) then  
            标记对象 p 为边界点或噪声点；  

         else  
            标记对象 p 为核心点，并建立新簇 C, 并将 p 邻域内所有点加入 C  
            for (NEps(p) 中所有尚未被处理的对象 q)  do  
                检查其 Eps 邻域 NEps(q)，若 NEps(q) 包含至少 MinPts 个对象，
                则将 NEps(q) 中未归入任何一个簇的对象加入 C；  
            end for  

        end if  
    end if  
 end for
```

**代码实现**
```py
def distance(data):
    '''计算样本点之间的距离
    :param data(mat):样本
    :return:dis(mat):样本点之间的距离
    '''
    m, n = np.shape(data)
    dis = np.mat(np.zeros((m, m)))
    for i in range(m):
        for j in range(i, m):
            # 计算i和j之间的欧式距离
            tmp = 0
            for k in range(n):
                tmp += (data[i, k] - data[j, k]) * (data[i, k] - data[j, k])
            dis[i, j] = np.sqrt(tmp)
            dis[j, i] = dis[i, j]
    return dis


def find_eps(distance_D, eps):
    '''找到距离≤eps的样本的索引
    :param distance_D(mat):样本i与其他样本之间的距离
    :param eps(float):半径的大小
    :return: ind(list):与样本i之间的距离≤eps的样本的索引
    '''
    ind = []
    n = np.shape(distance_D)[1]
    for j in range(n):
        if distance_D[0, j] <= eps:
            ind.append(j)
    return ind


def dbscan(data, eps, MinPts):
    '''DBSCAN算法
    :param data(mat):需要聚类的数据集
    :param eps(float):半径
    :param MinPts(int):半径内最少的数据点数
    :return:
        types(mat):每个样本的类型：核心点、边界点、噪音点
        sub_class(mat):每个样本所属的类别
    '''
    m = np.shape(data)[0]
    # 在types中，1为核心点，0为边界点，-1为噪音点
    types = np.mat(np.zeros((1, m)))
    sub_class = np.mat(np.zeros((1, m)))
    # 用于判断该点是否处理过，0表示未处理过
    dealt = np.mat(np.zeros((m, 1)))
    # 计算每个数据点之间的距离
    dis = distance(data)
    # 用于标记类别
    number = 1

    # 对每一个点进行处理
    for i in range(m):
        # 找到未处理的点
        if dealt[i, 0] == 0:
            # 找到第i个点到其他所有点的距离
            D = dis[i,]
            # 找到半径eps内的所有点
            ind = find_eps(D, eps)
            # 区分点的类型
            # 边界点
            if len(ind) > 1 and len(ind) < MinPts + 1:
                types[0, i] = 0
                sub_class[0, i] = 0
            # 噪音点
            if len(ind) == 1:
                types[0, i] = -1
                sub_class[0, i] = -1
                dealt[i, 0] = 1
            # 核心点
            if len(ind) >= MinPts + 1:
                types[0, i] = 1
                for x in ind:
                    sub_class[0, x] = number
                # 判断核心点是否密度可达
                while len(ind) > 0:
                    dealt[ind[0], 0] = 1
                    D = dis[ind[0],]
                    tmp = ind[0]
                    del ind[0]
                    ind_1 = find_eps(D, eps)
                    
                    if len(ind_1) > 1:  # 处理非噪音点
                        for x1 in ind_1:
                            sub_class[0, x1] = number
                        if len(ind_1) >= MinPts + 1:
                            types[0, tmp] = 1
                        else:
                            types[0, tmp] = 0
                            
                        for j in range(len(ind_1)):
                            if dealt[ind_1[j], 0] == 0:
                                dealt[ind_1[j], 0] = 1
                                ind.append(ind_1[j])
                                sub_class[0, ind_1[j]] = number
                number += 1
                
    # 最后处理所有未分类的点为噪音点
    ind_2 = ((sub_class == 0).nonzero())[1]
    for x in ind_2:
        sub_class[0, x] = -1
        types[0, x] = -1

    return types, sub_class


# types, P = dbscan(X, 0.1, 4)
# types, P
```

**优点**

- 相比 K-平均算法，DBSCAN 不需要预先声明聚类数量。
- DBSCAN 可以找出任何形状的聚类，甚至能找出一个聚类，它包围但不连接另一个聚类，另外，由于 MinPts 参数，single-link effect （不同聚类以一点或极幼的线相连而被当成一个聚类）能有效地被避免。
- DBSCAN 能分辨噪音（局外点）。
- DBSCAN 只需两个参数，且对数据库内的点的次序几乎不敏感（两个聚类之间边缘的点有机会受次序的影响被分到不同的聚类，另外聚类的次序会受点的次序的影响）。
- DBSCAN 被设计成能配合可加速范围访问的数据库结构，例如 R*树。
- 如果对资料有足够的了解，可以选择适当的参数以获得最佳的分类。

**缺点**

- DBSCAN 不是完全决定性的：在两个聚类交界边缘的点会视乎它在数据库的次序决定加入哪个聚类，幸运地，这种情况并不常见，而且对整体的聚类结果影响不大——DBSCAN 对核心点和噪音都是决定性的。DBSCAN* 是一种变化了的算法，把交界点视为噪音，达到完全决定性的结果。
- DBSCAN 聚类分析的质素受函数 regionQuery(P,ε) 里所使用的度量影响，最常用的度量是欧几里得距离，尤其在高维度资料中，由于受所谓“维数灾难”影响，很难找出一个合适的 ε ，但事实上所有使用欧几里得距离的算法都受维数灾难影响。
- 如果数据库里的点有不同的密度，而该差异很大，DBSCAN 将不能提供一个好的聚类结果，因为不能选择一个适用于所有聚类的 minPts-ε 参数组合。
- 如果没有对资料和比例的足够理解，将很难选择适合的 ε 参数。

## 聚类方法对比
![](https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/cluster/cluste-all-compare.png)

![](https://raw.githubusercontent.com/financialfly/pics/master/ml-notes/cluster/cluste-all-compare-2.png)


## sklearn
参考：[2.3. 聚类 · sklearn 中文文档](https://sklearn.apachecn.org/docs/0.21.3/22.html)

## 参考
1. [team-learning/Task5_cluster_plus.ipynb](https://github.com/datawhalechina/team-learning/blob/master/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/Task5_cluster_plus.ipynb)
