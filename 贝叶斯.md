贝叶斯
---------

## 目录
1. [示例](#示例)
2. [三种贝叶斯模型](#三种贝叶斯模型)
3. [常见问题](#常见问题)
4. [Python 实现](#Python-实现)
5. [参考](#参考)


## 示例
一个例子：现在分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，现在已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器 A 的概率是多少？

取一个球，是红球的概率为：

    P(B) = 8/20
    
这个球来自 A 容器的概率为：

    P(A) = 1/2
    
A 容器中红球的概率为：
    
    P(B|A) = 7/10

取出的球来自 A 容器且为红球的概率为：
    
    P(A|B) = P(A) * P(B|A) / P(B)
    
即：

    1/2 * 7/10 * 20/8 = 7/8 (0.875)
    

另一个例子：一座别墅在过去的20年里一共发生过 2 次被盗，别墅的主人有一条狗，狗平均每周晚上叫 3 次，在盗贼入侵时狗叫的概率被估计为 0.9，问题是：在狗叫时发生入侵的概率是多少？

将被盗定义成事件 B，则：

    P(B) = 2 / (20*365)
    
讲狗叫定义为事件 A，则：
    
    P(A) = 3/7
    
在盗贼入侵时狗叫的概率为 0.9

    P(A|B) = 0.9

在狗叫的时候发生入侵的概率为：

    P(B|A) = P(B) * P(A|B) / P(A)

即：
    
    2/(20*365)*0.9*7/3 = 0.00058
    
在以上两个例子中，P(B|A) = P(B) * P(A|B) / P(A) 即贝叶斯定理。

## 三种贝叶斯模型

1. 高斯分布朴素贝叶斯
高斯分布就是正态分布，用于一般分类问题。数据分布满足高斯分布，则效果很好，

2. 多项式分布朴素贝叶斯
多分布。如投掷骰子（6 个面概率差不多）

3. 伯努利分布
二分部，如硬币，正反两面的概率相差不大。

使用示例：
```py
# 独立性假设
# 假设属性之间没有关系

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.model_selection import train_test_split

import sklearn.datasets as datasets

iris = datasets.load_iris()
X = iris['data']
y = iris['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

gNB = GaussianNB()
gNB.fit(X_train, y_train)
gNB.score(X_test, y_test)

mNB = MultinomialNB()
mNB.fit(X_train, y_train)
mNB.score(X_test, y_test)
```

参数详解：
```
class sklearn.naive_bayes.GaussianNB(priors=None)
参数：
    priors:
        先验概率大小，如果没有给定，
        模型则根据样本数据自己计算（利用极大似然法）。
    var_smoothing：可选参数，所有特征的最大方差

属性：
    class_prior_:每个样本的概率
    class_count:每个类别的样本数量
    classes_:分类器已知的标签类型
    theta_:每个类别中每个特征的均值
    sigma_:每个类别中每个特征的方差
    epsilon_:方差的绝对加值方法
    
方法：
    # 贝叶斯的方法和其他模型的方法一致。
    fit(X,Y):在数据集(X,Y)上拟合模型。
    get_params():获取模型参数。
    predict(X):对数据集X进行预测。
    predict_log_proba(X):对数据集X预测，得到每个类别的概率对数值
    predict_proba(X):对数据集X预测，得到每个类别的概率。
    score(X,Y):得到模型在数据集(X,Y)的得分情况。
```


## 常见问题

### 极值问题情况下的每个类的分类概率
极值问题

很多时候遇到求出各种目标函数（object function）的最值问题（最大值或者最小值）。关于函数最值问题，其实在高中的时候我们就已经了解不少，最经典的方法就是：直接求出极值点。这些极值点的梯度为0。若极值点唯一，则这个点就是代入函数得出的就是最值；若极值点不唯一，那么这些点中，必定存在最小值或者最大值（去除函数的左右的最端点），所以把极值代入函数，经对比后可得到结果。

请注意：并不一定所有函数的极值都可以通过设置导数为0的方式求 出。也就是说，有些问题中当我们设定导数为0时，未必能直接计算出满足导数为0的点（比如逻辑回归模型），这时候就需要利用数值计算相关的技术（最典型为梯度下降法，牛顿法……）。

### 下溢问题如何解决
数值下溢问题：是指计算机浮点数计算的结果小于可以表示的最小数，因为计算机的能力有限，当数值小于一定数时，其无法精确保存，会造成数值的精度丢失，由上述公式可以看到，求概率时多个概率值相乘，得到的结果往往非常小；因此通常采用取对数的方式，将连乘转化为连加，以避免数值下溢。

### 零概率问题如何解决？
零概率问题，就是在计算实例的概率时，如果某个量x，在观察样本库（训练集）中没有出现过，会导致整个实例的概率结果是0.

在实际的模型训练过程中，可能会出现零概率问题（因为先验概率和反条件概率是根据训练样本算的，但训练样本数量不是无限的，所以可能出现有的情况在实际中存在，但在训练样本中没有，导致为0的概率值，影响后面后验概率的计算），即便可以继续增加训练数据量，但对于有些问题来说，数据怎么增多也是不够的。这时我们说模型是不平滑的，我们要使之平滑，一种方法就是将训练（学习）的方法换成贝叶斯估计。

现在看一个示例，及$P(敲声=清脆|好瓜=是)=\frac{8}{0}=0$
不论样本的其他属性如何，分类结果都会为“好瓜=否”，这样显然不太合理。

朴素贝叶斯算法的先天缺陷：其他属性携带的信息被训练集中某个分类下未出现的属性值“抹去”，造成预测出来的概率绝对为0。为了拟补这一缺陷，前辈们引入了拉普拉斯平滑的方法：对先验概率的分子(划分的计数)加1，分母加上类别数；对条件概率分子加1，分母加上对应特征的可能取值数量。这样在解决零概率问题的同时，也保证了概率和依然为1：
$$P(c) = \frac{{|{D_c}|}}{{|D|}} \to P(c) = \frac{{|{D_c}| + 1}}{{|D| + N}}$$
$$P({x_i}|c) = \frac{{|{D_{{x_i}|c}}|}}{{|{D_c}|}} \to P({x_i}|c) = \frac{{|{D_{{x_i}|c}}| + 1}}{{|{D_c}| + {N_i}}}$$

其中，N表示数据集中分类标签，$N_i$表示第$i$个属性的取值类别数，|D|样本容量，$|D_c|$表示类别c的记录数量，${|{D_{{x_i}|c}}|}$表示类别c中第i个属性取值为$x_i$的记录数量。

将这两个式子应用到上面的计算过程中，就可以弥补朴素贝叶斯算法的这一缺陷问题。

用西瓜的数据来看，当我们计算

P(好瓜=是)时，样本有17个，所以|D| = 17，N，好瓜标签可以分为｛是，否｝两类，所以N=2，（好瓜=是）的样本个数有8个，所以这里$|D_c|$=8。

综上，根据拉普拉斯平滑后有 $$P(好瓜=是) = \frac{{|{D_c}| + 1}}{{|D| + N}} = \frac{{|{8}| + 1}}{{|17| + 2}}$$
P（色泽=青绿|好瓜=是）时，色泽青绿的样本有8个，所以|D_c| = 8，N，色泽标签可以分为｛青绿，浅白，乌黑｝三类，所以N=3，（好瓜=是）的样本个数有3个，所以这里$|D_{c,x_i}|$=3。
综上，根据拉普拉斯平滑后有$$P（色泽=青绿|好瓜=是）= \frac{{|{D_{{x_i}|c}}| + 1}}{{|{D_c}| + {N_i}}}=\frac{{|{3}}| + 1}{{|{8}| + {3}}}$$
同理，分析可知，之前不合理的$P(敲声=清脆|好瓜=是)=\frac{8}{0}=0$在进行拉普拉斯平滑后为$$ P(敲声=清脆|好瓜=是)= \frac{{|{D_{{x_i}|c}}| + 1}}{{|{D_c}| + {N_i}}}=\frac{{|{0}}| + 1}{{|{8}| + {3}}}$$显然结果不是0，使结果变得合理。


## Python 实现
根据李航老师的代码构建自己的朴素贝叶斯模型。（参考：[lihang-code/4.NaiveBayes.ipynb](https://github.com/fengdu78/lihang-code/blob/master/%E7%AC%AC04%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/4.NaiveBayes.ipynb)）

这里采用GaussianNB 高斯朴素贝叶斯,概率密度函数为
$$P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )$$
数学期望：$\mu$
方差：$\sigma ^2=\frac{1}{n}\sum_i^n(x_i-\overline x)^2$
```py
import math

import numpy as np


# naive Bayes
class NaiveBayes:

    def __init__(self):
        self.model = None
 
    # 数学期望
    @staticmethod
    def mean(X):
        """计算均值
        Param: X : list or np.ndarray
        
        Return:
            avg : float
        
        """
        avg = 0.0
        # ========= show me your code ==================
        
        avg = np.mean(X)
        
        # ========= show me your code ==================
        return avg
 
    # 标准差（方差）
    def stdev(self, X):
        """计算标准差
        Param: X : list or np.ndarray
        
        Return:
            res : float
        
        """
        res = 0.0
        # ========= show me your code ==================
 
        # here
        res = math.sqrt(np.mean(np.square(X-self.mean(X))))
        # ========= show me your code ==================
        return res
        
    # 概率密度函数
    def gaussian_probability(self, x, mean, stdev):
        """根据均值和标注差计算x符号该高斯分布的概率
        Parameters:
        ----------
        x : 输入
        mean : 均值
        stdev : 标准差
        
        Return:
        
        res : float， x符合的概率值
            
        """
        res = 0.0
        # ========= show me your code ==================
 
        # here
        exp = math.exp(-math.pow(x - mean, 2) / 2 * math.pow(stdev, 2))
        res = (1 / (math.sqrt(2 * math.pi) * stdev)) * exp
        # ========= show me your code ==================
        
        return res
        
    # 处理X_train
    def summarize(self, train_data):
        """计算每个类目下对应数据的均值和标准差
        Param: train_data : list
        
        Return : [mean, stdev]
        """
        summaries = [0.0, 0.0]
        # ========= show me your code ==================
        
        # here
        summaries = [(self.mean(i), self.stdev(i)) for i in zip(*train_data)]
        # ========= show me your code ==================
        return summaries
 
    # 分类别求出数学期望和标准差
    def fit(self, X, y):
        labels = list(set(y))
        data = {label: [] for label in labels}
        for f, label in zip(X, y):
            data[label].append(f)
        self.model = {
            label: self.summarize(value) for label, value in data.items()
        }
        return 'gaussianNB train done!'
 
    # 计算概率
    def calculate_probabilities(self, input_data):
        """计算数据在各个高斯分布下的概率
        Paramter:
        input_data : 输入数据
        
        Return:
        probabilities : {label : p}
        """
        # summaries:{0.0: [(5.0, 0.37),(3.42, 0.40)], 1.0: [(5.8, 0.449),(2.7, 0.27)]}
        # input_data:[1.1, 2.2]
        probabilities = {}
        # ========= show me your code ==================
        for label, value in self.model.items():
            probabilities[label] = 1
            
            # here
            for i in range(len(value)):
                mean, stdev = value[i]
                probabilities[label] *= self.gaussian_probability(input_data[i], mean, stdev)
        # ========= show me your code ==================
        return probabilities
 
    # 类别
    def predict(self, X_test):
        # {0.0: 2.9680340789325763e-27, 1.0: 3.5749783019849535e-26}
        label = sorted(self.calculate_probabilities(X_test).items(), key=lambda x: x[-1])[-1][0]
        return label
    
    # 计算得分
    def score(self, X_test, y_test):
        right = 0
        for X, y in zip(X_test, y_test):
            label = self.predict(X)
            if label == y:
                right += 1
 
        return right / float(len(X_test))
```


## 参考
1. [team-learning/Task6_bayes_plus.ipynb](https://github.com/datawhalechina/team-learning/blob/master/%E5%88%9D%E7%BA%A7%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86/Task6_bayes_plus.ipynb)
2. [机器学习入门笔记06-朴素贝叶斯](https://blog.csdn.net/xyc_undermoon/article/details/104046444)
